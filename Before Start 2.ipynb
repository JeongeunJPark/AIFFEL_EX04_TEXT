{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시작하기 전에     \n",
    "\n",
    "#### 이 노드의 목표      \n",
    "\n",
    "   1. 텍스트용 데이터를 수치 데이터로 변환하는 과정을 이해한다.   \n",
    "   2. RNN에 대해 알아본다.         \n",
    "   3. 시퀀셜한 데이터를 다루는 방법을 이해한다.      \n",
    "   4. 1-D CNN에 대해 알아보고, 이를 이용하여 텍스트를 처리한다.       \n",
    "   5. IMDB와 네이버 영화리뷰 데이터셋을 이용하여 감성분류를 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 텍스트 데이터의 특징        \n",
    "\n",
    "1) 텍스트 감성분석의 기법     \n",
    "* 기계학습 기반의 접근법\n",
    "* 감성사전 기반 접근법   \n",
    "\n",
    "2) __워드 임베딩 기법__ ex)word2Vec using CNN, LSTM   \n",
    "  * 단어의 특성을 저차원 벡터값으로 표현하는 기법.    \n",
    "  * 단어의 의미가 유사할 경우 벡터 공간상에 가깝게 배치해 단어 간의 어휘적 관계를 벡터로 표현하는 방법.\n",
    "  * 라벨링 비용을 절감하면서, 정확도를 크게 향상시킬 수 있다.     \n",
    "  \n",
    "3) 텍스트 데이터를 어떻게 숫자로 바꿔보지?    \n",
    "\n",
    "    > \"텍스트를 어떻게 숫자 행렬로 표현할 것인가?\"\n",
    "    > \"텍스트의 입력 순서를 어떻게 인공지능에 반영해주는가?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 텍스트를 숫자 행렬로 표현하는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'feel', 'pretty']\n"
     ]
    }
   ],
   "source": [
    "sentences = ['I feel pretty', 'I really miss you', 'Have a nice day']\n",
    "\n",
    "word_list = 'I feel pretty'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'I', 4: 'feel', 5: 'pretty', 6: 'Have', 7: 'a', 8: 'nice', 9: 'day'}\n"
     ]
    }
   ],
   "source": [
    "#모든 문장을 단어 단위로 쪼개어 딕셔너리로 표현하기\n",
    "\n",
    "index_to_word = {}\n",
    "\n",
    "index_to_word[0]='<PAD>'\n",
    "index_to_word[1]='<BOS>'\n",
    "index_to_word[2]='<UNK>'\n",
    "index_to_word[3]='I'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='pretty'\n",
    "index_to_word[6]='Have'\n",
    "index_to_word[7]='a'\n",
    "index_to_word[8]='nice'\n",
    "index_to_word[9]='day'\n",
    "\n",
    "print(index_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'I': 3, 'feel': 4, 'pretty': 5, 'Have': 6, 'a': 7, 'nice': 8, 'day': 9}\n"
     ]
    }
   ],
   "source": [
    "#텍스트 데이터를 숫자로 바꾸려면, 단어(키):숫자(밸류) 구조여야 하므로 딕셔너리를 뒤집어보자.\n",
    "\n",
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 6, 327, 251]\n"
     ]
    }
   ],
   "source": [
    "#문장을 입력하면, 단어 인덱스 리스트로 변환하는 함수\n",
    "#인공지능 입력을 위해 모든 문장은 'BOS'로 시작해야한다.\n",
    "\n",
    "def get_changed_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 위의 목록에서 수동으로 word_to_index에 넣어준 'Have a nice day' 가 숫자로 변환되어 존재함을 알 수 있다.\n",
    "print(get_changed_sentence('Have a nice day', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 2, 2, 2], [1, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "#확장 ver(여러개의 문자를 적용시켜 보자)\n",
    "\n",
    "def get_changed_sentences(sentences, word_to_index):\n",
    "    return [get_changed_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "#기존에 sentences에 있던 문장들이 변환되어 존재함을 알 수 있음.\n",
    "#'I feel pretty', 'I really miss you', 'Have a nice day'\n",
    "#공통적으로 들어간 1(<BOS>) 말고 기존에 있었던 애들은 숫자 그대로, 없었던 애(really miss you)는 2로 통일되어 나오는 것을 알수있다.\n",
    "#왜 그럴까?\n",
    "changed_sentences = get_changed_sentences(sentences, word_to_index)\n",
    "print(changed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I <UNK> <UNK> <UNK>\n",
      "Have a nice day\n"
     ]
    }
   ],
   "source": [
    "#반대로, 숫자처리된 자료들을 다시 텍스트로 변환하는 함수도 만들어놓자.\n",
    "def get_back_sentence(changed_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in changed_sentence[1:])\n",
    "\n",
    "#2로 변환된 really miss you는 처리가 안된다!\n",
    "print(get_back_sentence([1,3,2,2,2], index_to_word))\n",
    "print(get_back_sentence([1,6,7,8,9], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I feel pretty', 'I <UNK> <UNK> <UNK>', 'Have a nice day']\n"
     ]
    }
   ],
   "source": [
    "#이번엔 숫자처리된 자료들을 텍스트(들)로 변환하는 확장ver.\n",
    "def get_back_sentences(changed_sentences, index_to_word):\n",
    "    return [get_back_sentence(changed_sentence, index_to_word) for changed_sentence in changed_sentences]\n",
    "\n",
    "print(get_back_sentences(changed_sentences, index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 임베딩 레이어        \n",
    "\n",
    "> 단순히 텍스트를 숫자로 대치하는 것 뿐만이 아니라, '텍스트 간의 상관관계'를 알 수 있어야 한다.    \n",
    "\n",
    "- 단어의 의미를 나타내는 벡터를 '파라미터'로 놓고 딥러닝이 이를 학습한다.\n",
    "- Tensorflow, Pytorch에는 이러한 의미 벡터들을 담아놓은 임베딩 레이어를 제공해준다.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __임베딩 레이어 사용상의 주의사항__    \n",
    " \n",
    " * 임베딩 레이어의 인풋 문장은 __길이가 일정해야 한다.__      \n",
    " * 임베딩 레이어에 집어넣을 때는 텐서플로우의 'keras.preprocessing.sequence.pad_sequence' 를 통해 <PAD> 를 주고 가자.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index 길이 :  10\n",
      "[[1 3 4 5 0]\n",
      " [1 3 2 2 2]\n",
      " [1 6 7 8 9]]\n",
      "tf.Tensor(\n",
      "[[[-0.03051151  0.02841732 -0.03729496 -0.02859491]\n",
      "  [ 0.0024193   0.03932982  0.00559451  0.0408089 ]\n",
      "  [ 0.04109034 -0.04591468 -0.03560702 -0.01259921]\n",
      "  [-0.03085756 -0.01416973  0.01689265  0.04038281]\n",
      "  [ 0.02616055 -0.02782708  0.0316637   0.04122571]]\n",
      "\n",
      " [[-0.03051151  0.02841732 -0.03729496 -0.02859491]\n",
      "  [ 0.0024193   0.03932982  0.00559451  0.0408089 ]\n",
      "  [ 0.01436741 -0.04632701 -0.04413111 -0.0402706 ]\n",
      "  [ 0.01436741 -0.04632701 -0.04413111 -0.0402706 ]\n",
      "  [ 0.01436741 -0.04632701 -0.04413111 -0.0402706 ]]\n",
      "\n",
      " [[-0.03051151  0.02841732 -0.03729496 -0.02859491]\n",
      "  [ 0.04573134  0.04993096 -0.04990758 -0.03642692]\n",
      "  [-0.00891285 -0.02764868  0.0483343  -0.01335718]\n",
      "  [-0.02255197  0.00026774  0.00019399 -0.02052029]\n",
      "  [-0.0233076   0.00730736  0.03960934  0.04454095]]], shape=(3, 5, 4), dtype=float32)\n",
      "\n",
      " output's shape :  (3, 5, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac23/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print(\"word_to_index 길이 : \", vocab_size)\n",
    "\n",
    "#단어가 4차원 워드벡터로 이루어질 것으로 가정\n",
    "word_vector_dimension = 4\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dimension, mask_zero=True)\n",
    "\n",
    "#raw_inputs를 말해주고,\n",
    "raw_inputs=np.array(get_changed_sentences(sentences, word_to_index))\n",
    "\n",
    "#프리프로세싱.시퀀스.패드시퀀스를 통해 길이를 패딩으로 맞춰줌\n",
    "#첫 센텐스의 마지막 부분이 0인 것을 알 수 있음\n",
    "raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs, value=word_to_index['<PAD>'], padding='post', maxlen=5)\n",
    "\n",
    "print(raw_inputs)\n",
    "\n",
    "#센텐스가 워드 벡터로 임베딩되어 output으로 나온다는 것을 알 수 있음\n",
    "output=embedding(raw_inputs)\n",
    "print(output)\n",
    "print(\"\\n output's shape : \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* output shape의 (3,5,4)는 각각     \n",
    "* 입력 문장의 개수, 입력 문장의 최대 길이, 워드벡터의 차원 수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시퀀스를 다루는 RNN      \n",
    "\n",
    "1) RNN의 특징    \n",
    "    - RNN(Recurrent Neural Network)/ recurrent:되풀이\n",
    "    - 텍스트 데이터, 음성 데이터와 같은 시퀀스 데이터를 처리하기에 좋다.\n",
    "    - 시퀀스 데이터? => 입력이 시간을 따라 발생하는 데이터.\n",
    "    - 나 오늘 ~했는데 그래서~ => 그 뒤를 아무도 예측할 수 없다!      \n",
    "  ---\n",
    "   - RNN은 이것에 대비해 __시간의 흐름에 따라 새롭게 변하는 데이터를 묘사하는 state machine으로 설계되었다__    \n",
    "    > state vs stateless\n",
    "    > state : 이전의 상태가 저장되어 다음의 입력에 영향을 미친다.   \n",
    "    > stateless : 이전 상태는 전혀 저장되지 않아 매번 새롭게 처음부터 끝까지 입력해줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) RNN을 사용한 텍스트 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "voca_size =10\n",
    "word_vector_dimension=4\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(voca_size, word_vector_dimension, input_shape=(None,)))\n",
    "\n",
    "#아까 많이 쓰인다고 했던 바로 그 레이어! LSTM\n",
    "model.add(keras.layers.LSTM(8))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아니, 근데 그러면 RNN만 되나?      __NOPE__     \n",
    "\n",
    "- 1-D CNN을 사용할 수도 있다.(이미지에는 2D CNN을 썼지만, 텍스트는 한줄로 들어오는 자료이므로 1D CNN)   \n",
    "- 1-D CNN은 문장 전체를 한꺼번에 길이 7짜리의 필터로 스캐닝하여 7단어 이내에서 특징을 추출하여 문장 분류   \n",
    "- CNN은 RNN보다 병렬처리를 효율적으로 수행하므로, 학습 속도가 빠르게 진행된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-D CNN을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 16)          464       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "voca_size = 10\n",
    "word_vector_dim = 4\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(voca_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16,7,activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16,7,activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8,activation='relu'))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__이 수많은 레이어 중 GlobalMaxPooling1D()만 써도 의외로 긍정적인 효과가 있을 수 있다!__ 고 한다   \n",
    "- 이 방법은 전체 문장 중 가장 중요한 한 단어만 추출하여 긍/부정을 판단하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "voca_size = 10\n",
    "word_vector_dim = 4\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(voca_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8,activation='relu'))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 외에도,     \n",
    "\n",
    "> - 1-D CNN을 RNN레이어와 섞어 쓰거나,    \n",
    "> - FFN(FeedForwardNetwork)레이어만으로 구성하거나,      \n",
    "> - Transformer 레이어를 쓰는 등 \n",
    "\n",
    "다양한 해결책을 찾을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB 영화 리뷰 감성분석 해보기       \n",
    "\n",
    "- IMDB Large Movie Dataset : 50,000개의 영어로 작성된 영화 리뷰 텍스트    \n",
    "- 긍정:1, 부정:0    \n",
    "- 훈련 데이터:25,000, 평가 데이터:25,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    }
   ],
   "source": [
    "#데이터 다운로드 및 전처리 시퀀스\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "#num_words=10,000 옵션을 통해, 10,000개의 word_to_index 딕셔너리가 생성된 형태로 데이터가 다운된다.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n"
     ]
    }
   ],
   "source": [
    "#1번째 리뷰데이터는 218개의 단어로 이루어져 있으며, \n",
    "#결과는 1이므로 긍정 리뷰임을 알 수 있음\n",
    "print(x_train[0]) \n",
    "print('라벨: ', y_train[0])  \n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n",
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "#텐서플로우 사용 가이드에 따라, 인코더 함수와 디코더 함수를 좀 수정한다.\n",
    "\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "#디코더 함수를 수정하기 전에는 1:the 였다는 것을 기억하자.\n",
    "print(index_to_word[1])     # 'the'  \n",
    "print(word_to_index['the'])  # 1 \n",
    "\n",
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있다.\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2 \n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "#위에서 함수를 수정하며 '세 칸 밀렸기 때문에' 4:the가 되었다.\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<BOS>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-3b7ce17c5f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#수정한 디코더가 제대로 작동하는지 확인해보자\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_changed_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'라벨: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1번째 리뷰데이터의 라벨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-16a1b208d945>\u001b[0m in \u001b[0;36mget_changed_sentence\u001b[0;34m(sentence, word_to_index)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_changed_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<BOS>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 위의 목록에서 수동으로 word_to_index에 넣어준 'Have a nice day' 가 숫자로 변환되어 존재함을 알 수 있다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '<BOS>'"
     ]
    }
   ],
   "source": [
    "#수정한 디코더가 제대로 작동하는지 확인해보자\n",
    "print(get_changed_sentence(x_train[5], index_to_word))\n",
    "print('라벨: ', y_train[5])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 잊지 말아야 할 것     \n",
    "   * __pad_sequences__ 를 통해 데이터의 문장 길이를 통일해야 한다.\n",
    "   * 문장 최대 길이를 정하는 __maxlen__ 도 모델 성능에 영향을 미치기 때문에,    \n",
    "   * 전체 데이터셋의 길이 분포를 확인하여 가장 적절한 길이를 적용해주는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 또, 패딩을 앞/뒤에 적용하는 __pre, post padding__ 도 모델의 성능에 영향을 미친다.     \n",
    "   * Pre padding이 좋음. 왜? >>     \n",
    "   * RNN은 입력 데이터가 순차적으로 적용되므로, 뒤로 갈 수록 state에 미치는 영향이 커지기 때문.    \n",
    "   * 따라서 뒤쪽 부분이 padding이 될 경우 비효율적이며, pre-padding을 적용하는 것이 좋다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "#Pre-padding과 Post-padding의 차이를 적용할 때는 요렇게! \n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 분석 실습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 훈련을 시행하기 전에, 훈련 데이터셋을 훈련+검증 셋으로 나누어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. RNN모델 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.6931 - accuracy: 0.5060 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.6930 - accuracy: 0.5124 - val_loss: 0.6931 - val_accuracy: 0.5007\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6929 - val_accuracy: 0.5007\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.6919 - accuracy: 0.5126 - val_loss: 0.6923 - val_accuracy: 0.5016\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 1s 47ms/step - loss: 0.6890 - accuracy: 0.5059 - val_loss: 0.6908 - val_accuracy: 0.5044\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 20 에포크 훈련시킨 후, 5에포크를 기점으로 성능에 큰 변화가 없다는 것을 알 수 있었다.               \n",
    "epochs=5  \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 7s - loss: 0.6910 - accuracy: 0.5070\n",
      "[0.6909895539283752, 0.5070000290870667]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# history 변수를 통해 epoch마다 발생한 수치들을 확인해 볼 수 있다.\n",
    "history_dict = history.history\n",
    "print(history_dict.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAta0lEQVR4nO3deXxU5dn/8c9FArILElBZAxZQdiGCKwa1imBBsTxKqYq2Ktaq6NNW+2irrQ+1Wn/Wh1Zr3bUg1NYNraKCAlZrFRQ0iChiRMSFoLIIlsXr98d9QoYwIXPITCbL9/16zWtm7rNd55Dky31Wc3dERERS1SDbBYiISO2i4BARkVgUHCIiEouCQ0REYlFwiIhILAoOERGJRcEhWWNmT5nZWekeN5vMrNjMjsvAfOea2Q+jz+PN7JlUxt2D5XQ2s41mlrOntUrdp+CQWKI/KqWvb8xsc8L38XHm5e4nuvt96R63JjKzn5vZ/CTteWa2xcz6pDovd5/m7senqa6dgs7dV7p7c3ffno75l1uWm9m30j1fqX4KDokl+qPS3N2bAyuB7yS0TSsdz8xys1dljfQX4HAz61qu/XTgTXcvykJNIntEwSFpYWaFZrbKzC43s0+Ae8ystZk9YWZrzOyL6HPHhGkSd79MMLN/mtmN0bjvm9mJezhuVzObb2YbzGy2md1iZlMrqDuVGq81sxej+T1jZnkJw88wsw/MbK2ZXVnR9nH3VcBzwBnlBp0J3FdZHeVqnmBm/0z4/m0ze9vM1pnZHwFLGHaAmT0X1VdiZtPMrFU07C9AZ+DxqMf4MzPLj3oGudE47c1sppl9bmbLzezchHlfY2YPmtn90bZZYmYFFW2DipjZ3tE81kTb8iozaxAN+5aZzYvWrcTM/hq1m5n93sw+i4a9EafXJlWj4JB02g/YB+gCnEf4+bon+t4Z2Az8cTfTDwGWAXnADcBdZmZ7MO4DwCtAG+Aadv1jnSiVGr8HnA20AxoBPwEws17An6L5t4+Wl/SPfeS+xFrMrCcwAJieYh27iELsIeAqwrZ4DzgicRTguqi+g4BOhG2Cu5/Bzr3GG5IsYjqwKpr+u8BvzOzYhOGjgBlAK2BmKjUn8Qdgb6AbcDQhTM+Ohl0LPAO0JmzbP0TtxwNDgR7Rsk8D1u7BsmVPuLteeu3RCygGjos+FwJbgMa7GX8A8EXC97nAD6PPE4DlCcOaAg7sF2dcwh/dbUDThOFTgakprlOyGq9K+P4jYFb0+ZfAjIRhzaJtcFwF824KrAcOj75PBh7bw231z+jzmcDLCeMZ4Q/9DyuY78nA68n+DaPv+dG2zCWEzHagRcLw64B7o8/XALMThvUCNu9m2zrwrXJtOcB/gF4JbecDc6PP9wO3Ax3LTXcM8A5wKNAg278L9e2lHoek0xp3/7r0i5k1NbM/R7sf1gPzgVZW8Rk7n5R+cPdN0cfmMcdtD3ye0AbwYUUFp1jjJwmfNyXU1D5x3u7+Fbv5X29U09+AM6Pe0XhCL2RPtlWp8jV44ncza2dmM8zso2i+Uwk9k1SUbssNCW0fAB0SvpffNo0t3vGtPEIv7oMKlvEzQhi+Eu0KOwfA3Z8j9G5uAT41s9vNrGWM5UoVKDgkncrfavm/gZ7AEHdvSdi1AAn74DPgY2AfM2ua0NZpN+NXpcaPE+cdLbNNJdPcB/wX8G2gBfBEFesoX4Ox8/peR/h36RfN9/vl5rm722OvJmzLFgltnYGPKqkpjhJgK2EX3S7LcPdP3P1cd29P6IncatGZWe4+xd0HAb0Ju6x+msa6ZDcUHJJJLQj76r80s32AqzO9QHf/AFgAXGNmjczsMOA7Garx78BJZnakmTUCfk3lv1MvAF8Sdr/McPctVazjH0BvMxsT/U//YsIuu1ItgI3RfDuw6x/XTwnHFnbh7h8CLwHXmVljM+sH/ACYlmz8FDWK5tXYzBpHbQ8Ck82shZl1AS4j9Iwws7EJJwl8QQi67WZ2iJkNMbOGwFfA14TdalINFBySSTcDTQj/q3wZmFVNyx0PHEbYbfS/wF8J+9GTuZk9rNHdlwAXEg7Gf0z4w7aqkmmcsN++S/RepTrcvQQYC/yWsL7dgRcTRvkVMBBYRwiZh8vN4jrgKjP70sx+kmQR4wjHPVYDjwBXu/uzqdRWgSWEgCx9nQ1cRPjjvwL4J2F73h2NfwjwbzPbSDj4fom7vw+0BO4gbPMPCOt+YxXqkhgsOtAkUmdFp3C+7e4Z7/GI1AfqcUidE+3GOMDMGpjZcGA08GiWyxKpM3R1r9RF+xF2ybQh7Dq6wN1fz25JInWHdlWJiEgs2lUlIiKx1ItdVXl5eZ6fn5/tMkREapWFCxeWuHvb8u31Ijjy8/NZsGBBtssQEalVzOyDZO3aVSUiIrEoOEREJBYFh4iIxFIvjnGISPXYunUrq1at4uuvv658ZKkxGjduTMeOHWnYsGFK4ys4RCRtVq1aRYsWLcjPz6fiZ3BJTeLurF27llWrVtG1a/knGyenXVUikjZff/01bdq0UWjUImZGmzZtYvUSFRwiklYKjdon7r+ZdlVVYNo0uPRSWLMG9t4bjjkG+vaF0u1b298zNe/mzcN26tYNGui/JSJ1koIjiWnT4LzzYFP08NF16+CRR8JLUtO8OfTrBwMGQP/+4b1PH2jatLIpRfbc2rVrOfbYYwH45JNPyMnJoW3bcOHzK6+8QqNGjSqcdsGCBdx///1MmTJlt8s4/PDDeemll6pc69y5c7nxxht54oknKh+5hlFwJHHllWWhkahLFyguhtL7QtaV93TO64sv4I03YNEiWLwYpk6FW28Nwxo0gB49dg6TAQNgv8Tn1Um9Mm1a+H1buRI6d4bJk2H8+D2fX5s2bVi0aBEA11xzDc2bN+cnPyl7PtW2bdvIzU3+Z6+goICCgoJKl5GO0KjtFBxJrFy5+/Zku32kTOLvnnsI28WLQ5gsWgQvvwwzZpSN067drmHSowdU8PstdUT5nv0HH4TvULXwKG/ChAnss88+vP766wwcOJDTTjuNSZMmsXnzZpo0acI999xDz549d+oBXHPNNaxcuZIVK1awcuVKJk2axMUXXwxA8+bN2bhxI3PnzuWaa64hLy+PoqIiBg0axNSpUzEznnzySS677DLy8vIYOHAgK1asSLlnMX36dH7zm9/g7owcOZLrr7+e7du384Mf/IAFCxZgZpxzzjlceumlTJkyhdtuu43c3Fx69erFjMRfrAzSr2YSnTuHH+Jk7RKPGXTtGl4nn1zW/uWXZT2T0t7J//0fbImewN24cdi1lRgm/fpBy5bVvQaSKcl69ps2hfZ0BgfAO++8w+zZs8nJyWH9+vXMnz+f3NxcZs+ezf/8z//w0EMP7TLN22+/zfPPP8+GDRvo2bMnF1xwwS7XObz++ussWbKE9u3bc8QRR/Diiy9SUFDA+eefz/z58+natSvjxo1Luc7Vq1dz+eWXs3DhQlq3bs3xxx/Po48+SqdOnfjoo48oKioC4MsvvwTgt7/9Le+//z577bXXjrbqkNHDl2Y23MyWmdlyM7uignEKzWyRmS0xs3kJ7ZeYWVHUPimh/VozeyOa5hkza5/uuidP3nVffNOmoV3So1UrGDoULr4Y7r4bFi6EjRvhzTfhL3+BCy8M4zz2GFx0ERx1VDhJ4YADYMwY+PWvYebMEPB6pEztVFnPPp3Gjh1LTk4OAOvWrWPs2LH06dOHSy+9lCVLliSdZuTIkey1117k5eXRrl07Pv30013GGTx4MB07dqRBgwYMGDCA4uJi3n77bbp167bjmog4wfHqq69SWFhI27Ztyc3NZfz48cyfP59u3bqxYsUKLrroImbNmkXL6H9Q/fr1Y/z48UydOrXCXXCZkLElmVkOcAvwbcJT2F41s5nu/lbCOK2AW4Hh7r7SzNpF7X2Ac4HBwBZglpn9w93fBX7n7r+IxrsY+CUwMZ21l/5vJ537XqVyDRuGXkafPvD974c2d/j44517JosWwaOPlgVGq1Y790z694devWCvvbKwEpKy6uzZN2vWbMfnX/ziFwwbNoxHHnmE4uJiCgsLk06zV8IPUE5ODtu2bUtpnKo8HK+iaVu3bs3ixYt5+umnueWWW3jwwQe5++67+cc//sH8+fOZOXMm1157LUuWLKmWAMnkEgYDy919BYCZzSA8+/mthHG+Bzzs7isB3P2zqP0g4GV33xRNOw84BbjB3dcnTN8MyMj/N8ePV1DUBGbQvn14jRhR1v7VV6F3khgmd9xRtusjNxcOOmjnMOnfH/Lyqn8dJLnJk3c+xgHV07Nft24dHTp0AODee+9N+/wPPPBAVqxYQXFxMfn5+fz1r39NedohQ4ZwySWXUFJSQuvWrZk+fToXXXQRJSUlNGrUiFNPPZUDDjiACRMm8M033/Dhhx8ybNgwjjzySB544AE2btxIq1at0r5O5WUyODoAHyZ8XwUMKTdOD6Chmc0FWgD/5+73A0XAZDNrA2wGRgA7HqhhZpOBM4F1wLBkCzez84DzADrr4ETGpfvsmMo0awaHHhpepbZvh/fe2zlM5swJu75Kdeiwc5gMGBB2f+mak+qXrZ79z372M8466yxuuukmjjnmmLTPv0mTJtx6660MHz6cvLw8Bg8eXOG4c+bMoWPHjju+/+1vf+O6665j2LBhuDsjRoxg9OjRLF68mLPPPptvvvkGgOuuu47t27fz/e9/n3Xr1uHuXHrppdUSGpDBZ46b2VjgBHf/YfT9DGCwu1+UMM4fgQLgWKAJ8C9gpLu/Y2Y/AC4ENhJ6KZvd/dJyy/g50Njdr95dLQUFBa4HOWVO+bNjIPzP8fbba0avbc2aECSJZ3YtXRqCBkIIlb/mpG9fXXOyJ5YuXcpBBx2U7TKybuPGjTRv3hx358ILL6R79+5ceumllU+YRcn+7cxsobvvco5yJnscq4BOCd87AquTjFPi7l8BX5nZfKA/8I673wXcBWBmv4nGLe8B4B/AboNDMqs6z47ZE23bwnHHhVepr7+Gt97aOUweeAD+9Kcw3GzXa07694f999dp2FK5O+64g/vuu48tW7Zw8MEHc/7552e7pLTKZI8jF3iH0Jv4CHgV+J67L0kY5yDgj8AJQCPgFeB0dy8ys3bu/pmZdQaeAQ5z9y/MrHt0kBwzuwg42t2/u7ta1OPIrAYNkp/ZZAZRz7pWcA8HaxPDZPFieP/9snHatt01THr2DAf2RT2O2qxG9DjcfZuZ/Rh4GsgB7nb3JWY2MRp+m7svNbNZwBvAN8Cd7l4UzeKh6BjHVuBCd/8iav+tmfWMxv+ANJ9RJfHVletezCA/P7xGjy5rX7du12tO/vAH+M9/wvC99tr5mpPSA/F7713tqyBSLTLW46hJ1OPIrJp+jCMTtm2DZct2PU14zZqycbp23TlMBgwIt62py7u61OOovWpEj0Pqj/p43UtuLvTuHV6l61l6zUlpiJS+P/ZY2a68vffeOUwKCna+67JIbaDgkLTQdS87X3Ny4oll7V99BUVFO4fJXXeFdgi9kDFjwuvww3VqsNR8+hEVybBmzWDIEDj//HCn4JdegvXr4Z13wu1W+vaFW24Jt1Xp0AEuuABmz4atW7Ndee1TWFjI008/vVPbzTffzI9+9KPdTlO6K3vEiBFJ7/l0zTXXcOONN+522Y8++ihvvVV2ffMvf/lLZs+eHaP65ObOnctJJ51U5fmkk4JDJAsaNIDu3eHss+Hxx8OxkenT4cgj4f774dvfDrebLx0e46me9dq4ceN2uUPsjBkzUr5f1JNPPrnHF9GVD45f//rXHJd4DngdouAQqQFatoTTT4e//Q1KSsJDw0aODO+jRoXTgMeNC8M3bsx2tTXXd7/7XZ544gn+E53yVlxczOrVqznyyCO54IILKCgooHfv3lx9dfJLv/Lz8ykpKQFg8uTJ9OzZk+OOO45ly5btGOeOO+7gkEMOoX///px66qls2rSJl156iZkzZ/LTn/6UAQMG8N577zFhwgT+/ve/A+EK8YMPPpi+fftyzjnn7KgvPz+fq6++moEDB9K3b1/efvvtlNd1+vTp9O3blz59+nD55ZcDsH37diZMmECfPn3o27cvv//97wGYMmUKvXr1ol+/fpx++ukxt+qudIxDpIZp0iTcgv7kk8Nt5p9/Hh56KNzYccaMcMv5E06AU0+Fk06C1q2zXHAFJk0Kx3PSacAAuPnmioe3adOGwYMHM2vWLEaPHs2MGTM47bTTMDMmT57MPvvsw/bt2zn22GN544036NevX9L5LFy4kBkzZvD666+zbds2Bg4cyKBBgwAYM2YM5557LgBXXXUVd911FxdddBGjRo3ipJNO4rvf3fmysq+//poJEyYwZ84cevTowZlnnsmf/vQnJk2aBEBeXh6vvfYat956KzfeeCN33nlnpdsh27dfV49DpAZr1CiExO23hzO25s6Fc8+FBQvgzDPDQ7CGDw/DP/us0tnVC4m7qxJ3Uz344IMMHDiQgw8+mCVLluy0W6m8F154gVNOOYWmTZvSsmVLRo0atWNYUVERRx11FH379mXatGkV3pa91LJly+jatSs9evQA4KyzzmL+/Pk7ho8ZMwaAQYMGUVxcnNI6Zvv26+pxiNQSOTlw9NHhdfPN8Oqr8PDDoTdy/vnhoPqRR4aeyCmnQKdOlc4yo3bXM8ikk08+mcsuu4zXXnuNzZs3M3DgQN5//31uvPFGXn31VVq3bs2ECRP4upIDR1bBOdITJkzg0UcfpX///tx7773MnTt3t/Op7Fq50luzV3Tr9jjzrK7br6vHIVILNWgQztS6/np4991wmu9VV8Hnn8Mll4RraYYMgRtugOXLs11t9WrevDmFhYWcc845O3ob69evp1mzZuy99958+umnPPXUU7udx9ChQ3nkkUfYvHkzGzZs4PHHH98xbMOGDey///5s3bqVadOm7Whv0aIFGzZs2GVeBx54IMXFxSyP/iH+8pe/cPTRR1dpHYcMGcK8efMoKSlh+/btTJ8+naOPPpqSkhK++eYbTj31VK699lpee+21nW6/fsMNN/Dll1+ysYoHytTjEKnlzMLdffv1g1/9KpzmW9oTufzy8OrXL1wncuqp4aLFun7B4bhx4xgzZsyOXVb9+/fn4IMPpnfv3nTr1o0jjjhit9OXPpt8wIABdOnShaOOOmrHsGuvvZYhQ4bQpUsX+vbtuyMsTj/9dM4991ymTJmy46A4QOPGjbnnnnsYO3Ys27Zt45BDDmHixHh3Sqppt1/XLUdE6rAPPghnZj30ELz4YriCvXv3ECBjxoQr19MZIrrlSO0V55Yj2lUlUod16RLObnrhBVi9Otw2Pj8ffvc7GDx45+GlzycRqYyCQ6Se2G8/mDgRnnkmnIF1773h9NbbboOhQ8NV66XDddW67I6CQ6Qe2mcfOOssmDkzXLU+Y0Y4W2vq1HD6b7t2ZcM3b4437/qw+7uuiftvpuAQqedatIDTToO//jWEyGOPhavVZ84MzyVp27ZseJKThnbSuHFj1q5dq/CoRdydtWvX0rhx45Sn0cFxEUlq69Zw1frDD4cD7J99Fh5adcIJ4cD6qFG7XrW+detWVq1aVek1ElKzNG7cmI4dO9Kw3KMsKzo4ruAQkUpt3x7u6vvQQyFIPvwwPJNk2LBwhtbo0eEYitQtOqtKpAaZNi2c3dSgQXhPuI6sRsrJCbd9v/nmcIrvK6/Af/93eB77xInhGSRDh4bhK1dmu1rJNPU4RKpZXXrUrnt4SFXpBYdvvhnaCwrKrhWJbtEktZB2VSk4pIbIzw//ay+vSxdI8R53Nda774YQefjh0CsB6NOnLET0mNzaRcGh4JAaokGDsmeQJzKD6E4RdcKHH5Zdtf7CC2Gdv/WtslufHHKIQqSm0zEOkRqic+d47bVVp05w8cUwb164Jfyf/wzdusFNN4UbMHbuHG7IOG+erlqvbRQcItVs8uRwTCNR06ahva7ad99wXOfpp8NpvffdB4MGheM6hYWw//5lw7dsyXa1UhkFh0g1Gz8+/MHs0iXsqunSpXYeGN9TrVuHh1A9+mi44PDBB+GYY8Iz14cPDyFz5pnhQsS4V61L9dAxDhGpEb7+Gp59NhxYf+wx+OILaNYMRowIx0VGjgxXuUv10TEOEanRGjeG73wH7rkHPv00hMgZZ8D8+TBuXLj1ySmnwCefZLtSUXCISI3TsCEcd1y4DfxHH4Wzsn70oxAmw4aFg+2SPQoOEanRcnLCs9Rvugmeeiqc5qvwyC4Fh4jUGkcdFcJj1SqFRzYpOESkVjnqKJg1K+zCKiwMTzaU6qXgEJFa58gjQ3isXh16HgqP6qXgEJFa6YgjysKjsDD0QKR6KDhEpNY64ohwtfknn4Seh8Kjeig4RKRWO/zwsvAoLAwHziWzFBwiUusddlgIj08/VXhUBwWHiNQJhx0GzzwT7n9VWBiu95DMUHCISJ1x6KEKj+qg4BCROmXIkBAeJSUhPPQM9PRTcIhInTNkSLiv1dq1Co9MyGhwmNlwM1tmZsvN7IoKxik0s0VmtsTM5iW0X2JmRVH7pIT235nZ22b2hpk9YmatMrkOIlI7DR4cwuPzz0N4JHvOu+yZjAWHmeUAtwAnAr2AcWbWq9w4rYBbgVHu3hsYG7X3Ac4FBgP9gZPMrHs02bNAH3fvB7wD/DxT6yAitdshhyg8MiGTPY7BwHJ3X+HuW4AZwOhy43wPeNjdVwK4+2dR+0HAy+6+yd23AfOAU6JxnonaAF4GOmZwHUSkljvkEJg9G778MoRHcXGWC6oDMhkcHYDEcxpWRW2JegCtzWyumS00szOj9iJgqJm1MbOmwAigU5JlnAM8lWzhZnaemS0wswVr1qyp0oqISO1WUKDwSKdMBoclaSv/nNpcYBAwEjgB+IWZ9XD3pcD1hN1Ss4DFwLbECc3syqhtWrKFu/vt7l7g7gVt27at0oqISO03aFAIj/XrFR5VlcngWMXOvYSOQPl7WK4CZrn7V+5eAswnHNPA3e9y94HuPhT4HHi3dCIzOws4CRjv9eGh6SKSFonhcfTR8P772a6odspkcLwKdDezrmbWCDgdmFlunMeAo8wsN9olNQRYCmBm7aL3zsAYYHr0fThwOeGA+qYM1i8iddDAgSE8NmwIPY8VK7JdUe2TseCIDmD/GHiaEAYPuvsSM5toZhOjcZYSdkW9AbwC3OnuRdEsHjKzt4DHgQvd/Yuo/Y9AC+DZ6DTe2zK1DiJSM0ybBvn50KBBeJ+WdAd16gYOhDlzYONGhceesPqwp6egoMAXLFiQ7TJEZA9MmwbnnQebEvYvNG0Kt98O48dXbd6LFsGxx0KzZvD883DAAVWbX11jZgvdvaB8u64cF5Ea7cordw4NCN+vvLLq8x4wIPQ8vvoq9Dzee6/q86wPFBwiUqNVdLuQdN1GZMAAeO452Lw5hMfy5emZb12m4BCRGq1z53jte6J//9DzUHikRsEhIjXa5MnhmEaipk1Dezr17x96Hv/5TzhV9913K5+mvlJwiEiNNn58OBDepQuYhfd0HBhPpl+/EB5btoSexzvvpH8ZdYHOqhIRKefNN+GYY6BhQ5g7F3r0yHZF2aGzqkREUtS3bzg9d9u20PNYtizbFdUsCg4RkST69Am7rbZtg2HDFB6JFBwiIhXo0yf0PLZvDz2Pt9/OdkU1g4JDRGQ3evcO4eEeeh4KDwWHiEilevUqC4/CQli6NNsVZZeCQ0QkBQcdFMIDQs/jrbeyW082KThERFJ00EHh9Fyz+h0eCg4RkRgOPDD0PBo0COGxZEm2K6p+Cg4RkZgOPDD0PHJy6md4KDhERPZAz56h55GbG8KjqKjyaeoKBYeIyB7q2TP0PBo2DLcoqS/hoeAQEamCHj3KwmPYsHCfq7pOwSEiUkXdu4fw2Guv0PN4441sV5RZCg4RkTSoT+Gh4BARSZNvfSuER5MmITwWL852RZmh4BARSaPE8Dj22LoZHgoOEZE0O+CAEB5Nm4aex6JF2a4ovRQcIiIZUBoezZqFnsfrr2e7ovRRcIiIZEi3biE8mjevW+Gh4BARyaDS8GjRIoTHa69lu6KqU3CIiGRY164hPFq2hOOOq/3hoeAQEakGieFx7LGwcGG2K9pzCg4RkWqSnx/Co1Wr0PNYsCDLBe0hBYeISDVKDI9vf7t2hkdKwWFmzcysQfS5h5mNMrOGmS1NRKRu6tIlhEfr1qHn8eqr2a4onlR7HPOBxmbWAZgDnA3cm6miRETqutLw2Gef0PN45ZVsV5S6VIPD3H0TMAb4g7ufAvTKXFkiInVf584hPNq0qV3hkXJwmNlhwHjgH1FbbmZKEhGpP0rDIy8vhMe//53tiiqXanBMAn4OPOLuS8ysG/B8xqoSEalHOnUK4dG2LRx/PLz8crYr2r2UgsPd57n7KHe/PjpIXuLuF2e4NhGReqM2hUeqZ1U9YGYtzawZ8BawzMx+mtnSRETql44dQ3jsu28Ij3/9K9sVJZfqrqpe7r4eOBl4EugMnJGpokRE6qvE8DjhBHjppWxXtKtUg6NhdN3GycBj7r4V8IxVJSJSj3XoEMJjv/1qZnikGhx/BoqBZsB8M+sCrK9sIjMbbmbLzGy5mV1RwTiFZrbIzJaY2byE9kvMrChqn5TQPjZq+8bMClKsX0SkVunQAZ5/HvbfP4THiy9mu6IyqR4cn+LuHdx9hAcfAMN2N42Z5QC3ACcSrvkYZ2a9yo3TCrgVGOXuvYGxUXsf4FxgMNAfOMnMukeTFRGuJ5mf2iqKiNROpT2P9u1h+HD45z+zXVGQ6sHxvc3sJjNbEL3+H6H3sTuDgeXuvsLdtwAzgNHlxvke8LC7rwRw98+i9oOAl919k7tvA+YBp0TjLHX3ZSmtnYhILde+feh51KTwSHVX1d3ABuC/otd64J5KpukAfJjwfVXUlqgH0NrM5prZQjM7M2ovAoaaWRszawqMADqlWCsAZnZeadCtWbMmzqQiIjVK+/ah59GxYwiPF17Ibj2pBscB7n511HtY4e6/ArpVMo0laSt/QD0XGASMBE4AfmFmPdx9KXA98CwwC1gMbEux1rAg99vdvcDdC9q2bRtnUhGRGmf//UPPo1MnOPFEmJ/FnfWpBsdmMzuy9IuZHQFsrmSaVezcS+gIrE4yzix3/8rdSwjHLfoDuPtd7j7Q3YcCnwPvpliriEidlBgeI0ZkLzxSDY6JwC1mVmxmxcAfgfMrmeZVoLuZdTWzRsDpwMxy4zwGHGVmudEuqSHAUgAzaxe9dyYcDJ+eYq0iInXWfvuF8OjcOfQ85s2rfJp0S/WsqsXu3h/oB/Rz94OBYyqZZhvwY+BpQhg8GN3naqKZTYzGWUrYFfUG8Apwp7sXRbN4yMzeAh4HLnT3LwDM7BQzWwUcBvzDzJ6Ot8oiIrVbaXh06RJ6HnPnVu/yzX3PruMzs5Xu3jnN9WREQUGBL6iNj9kSEdmNTz+FY46B4mJ44gkYttuLJOIzs4Xuvsv1clV5dGyyg98iIlJN9t0XnnsuPI525MjwuTpUJTh0yxERkSzbd9+w26pbNzjpJJgzJ/PL3G1wmNkGM1uf5LUBaJ/58kREpDLt2oXexgEHVE947DY43L2Fu7dM8mrh7noCoIhIDVEaHt27h/CYPTtzy6rKrioREalB2rYNvY3u3eE734Fnn83MchQcIiJ1SNu2oefRoweMGpWZU3UVHCIidUxeXuh5jBwZeh/ppuMUIiJ1UF4e/P3vmZm3ehwiIhKLgkNERGJRcIiISCwKDhERiUXBISIisSg4REQkFgWHiIjEouAQEZFYFBwiIhKLgkNERGJRcIiISCwKDhERiUXBISIisSg4REQkFgWHiIjEouAQEZFYFBwiIhKLgkNERGJRcIiISCwKDhERiUXBISIisSg4REQkFgWHiIjEouAQEZFYFBwiIhKLgkNERGJRcIiISCwKDhERiUXBISIisSg4REQkFgWHiIjEktHgMLPhZrbMzJab2RUVjFNoZovMbImZzUtov8TMiqL2SQnt+5jZs2b2bvTeOpPrICIiO8tYcJhZDnALcCLQCxhnZr3KjdMKuBUY5e69gbFRex/gXGAw0B84ycy6R5NdAcxx9+7AnOi7iIhUk0z2OAYDy919hbtvAWYAo8uN8z3gYXdfCeDun0XtBwEvu/smd98GzANOiYaNBu6LPt8HnJy5VRARkfIyGRwdgA8Tvq+K2hL1AFqb2VwzW2hmZ0btRcBQM2tjZk2BEUCnaNi+7v4xQPTeLtnCzew8M1tgZgvWrFmTplUSEZHcDM7bkrR5kuUPAo4FmgD/MrOX3X2pmV0PPAtsBBYD2+Is3N1vB24HKCgoKL9cERHZQ5nscayirJcA0BFYnWScWe7+lbuXAPMJxzRw97vcfaC7DwU+B96NpvnUzPYHiN4/Q0REqk0mg+NVoLuZdTWzRsDpwMxy4zwGHGVmudEuqSHAUgAzaxe9dwbGANOjaWYCZ0Wfz4rmISIi1SRju6rcfZuZ/Rh4GsgB7nb3JWY2MRp+W7RLahbwBvANcKe7F0WzeMjM2gBbgQvd/Yuo/bfAg2b2A2Al0ZlYIiJSPcy97u/+Lygo8AULFmS7DBGRWsXMFrp7Qfl2XTkuIiKxKDhERCQWBYeIiMSi4BARkVgUHCIiEouCQ0REYlFwiIhILAoOERGJRcEhIiKxKDhERCQWBYeIiMSi4BARkVgUHCIiEouCQ0REYlFwiIhILAoOERGJRcEhIiKxKDhERCQWBYeIiMSi4BARkVgUHCIiEouCQ0REYlFwiIhILAoOERGJRcEhIiKxKDhERCQWBYeIiMSi4BARkVgUHCIiEouCQ0REYlFwiIhILAoOERGJRcEhIiKxKDhERCQWBYeIiMSi4BARkVgUHCIiEouCQ0REYlFwiIjUQdOmQX4+NGgQ3qdNS9+8MxocZjbczJaZ2XIzu6KCcQrNbJGZLTGzeQntl0ZtRWY23cwaR+39zexfZvammT1uZi0zuQ4iIrXNtGlw3nnwwQfgHt7POy994ZGx4DCzHOAW4ESgFzDOzHqVG6cVcCswyt17A2Oj9g7AxUCBu/cBcoDTo8nuBK5w977AI8BPM7UOIiK10ZVXwqZNO7dt2hTa0yGTPY7BwHJ3X+HuW4AZwOhy43wPeNjdVwK4+2cJw3KBJmaWCzQFVkftPYH50edngVMzVL+ISK20cmW89rgyGRwdgA8Tvq+K2hL1AFqb2VwzW2hmZwK4+0fAjcBK4GNgnbs/E01TBIyKPo8FOiVbuJmdZ2YLzGzBmjVr0rJCIiK1QefO8drjymRwWJI2L/c9FxgEjAROAH5hZj3MrDWhd9IVaA80M7PvR9OcA1xoZguBFsCWZAt399vdvcDdC9q2bVv1tRERqSUmT4amTXdua9o0tKdDbnpmk9Qqdu4NdKRsd1PiOCXu/hXwlZnNB/pHw9539zUAZvYwcDgw1d3fBo6P2nsQQkdERCLjx4f3K68Mu6c6dw6hUdpeVZnscbwKdDezrmbWiHBwe2a5cR4DjjKzXDNrCgwBlhJ2UR1qZk3NzIBjo3bMrF303gC4Crgtg+sgIlIrjR8PxcXwzTfhPV2hARnscbj7NjP7MfA04ayou919iZlNjIbf5u5LzWwW8AbwDXCnuxcBmNnfgdeAbcDrwO3RrMeZ2YXR54eBezK1DiIisitzL3/Yoe4pKCjwBQsWZLsMEZFaxcwWuntB+XZdOS4iIrEoOEREJBYFh4iIxFIvjnGY2Rrggz2cPA8oSWM56aK64lFd8aiueGpqXVC12rq4+y4XwtWL4KgKM1uQ7OBQtqmueFRXPKornppaF2SmNu2qEhGRWBQcIiISi4KjcrdXPkpWqK54VFc8qiuemloXZKA2HeMQEZFY1OMQEZFYFBwiIhKLggMws7vN7DMzK6pguJnZlOjZ6W+Y2cAaUlehma2Lntm+yMx+WU11dTKz581safRc+EuSjFPt2yzFuqp9m5lZYzN7xcwWR3X9Ksk42dheqdSVlZ+xaNk5Zva6mT2RZFhWfidTqCtbv5PFZvZmtMxdbsyX9u3l7vX+BQwFBgJFFQwfATxFeDjVocC/a0hdhcATWdhe+wMDo88tgHeAXtneZinWVe3bLNoGzaPPDYF/A4fWgO2VSl1Z+RmLln0Z8ECy5WfrdzKFurL1O1kM5O1meFq3l3ocgLvPBz7fzSijgfs9eBloZWb714C6ssLdP3b316LPGwjPSin/WOBq32Yp1lXtom2wMfraMHqVPyslG9srlbqywsw6Eh7SdmcFo2TldzKFumqqtG4vBUdqUnl+erYcFu1qeMrMelf3ws0sHziY8L/VRFndZrupC7KwzaLdG4uAz4Bn3b1GbK8U6oLs/IzdDPyM8JyeZLL183Uzu68LsrO9HHjGzBaa2XlJhqd1eyk4UpPK89Oz4TXCvWT6A38AHq3OhZtZc+AhYJK7ry8/OMkk1bLNKqkrK9vM3be7+wDCI5QHm1mfcqNkZXulUFe1by8zOwn4zN0X7m60JG0Z3V4p1pWt38kj3H0gcCJwoZkNLTc8rdtLwZGaVJ6fXu3cfX3prgZ3fxJoaGZ51bFsM2tI+OM8zd0fTjJKVrZZZXVlc5tFy/wSmAsMLzcoqz9jFdWVpe11BDDKzIqBGcAxZja13DjZ2F6V1pWtny93Xx29fwY8AgwuN0pat5eCIzUzgTOjMxMOBda5+8fZLsrM9jMziz4PJvx7rq2G5RpwF7DU3W+qYLRq32ap1JWNbWZmbc2sVfS5CXAc8Ha50bKxvSqtKxvby91/7u4d3T0fOB14zt2/X260at9eqdSVpZ+vZmbWovQzcDxQ/kzMtG6vjD1zvDYxs+mEsyHyzGwVcDXhQCHufhvwJOGshOXAJuDsGlLXd4ELzGwbsBk43aNTKDLsCOAM4M1o/zjA/wCdE2rLxjZLpa5sbLP9gfvMLIfwh+RBd3/CzCYm1JWN7ZVKXdn6GdtFDdheqdSVje21L/BIlFe5wAPuPiuT20u3HBERkVi0q0pERGJRcIiISCwKDhERiUXBISIisSg4REQkFgWHSBWY2XYruxPqIjO7Io3zzrcK7owskk26jkOkajZHt+wQqTfU4xDJAAvPR7jewvMuXjGzb0XtXcxsjoVnIswxs85R+75m9kh0c7zFZnZ4NKscM7vDwvMynomu8MbMLjazt6L5zMjSako9peAQqZom5XZVnZYwbL27Dwb+SLirKtHn+929HzANmBK1TwHmRTfHGwgsidq7A7e4e2/gS+DUqP0K4OBoPhMzs2oiyenKcZEqMLON7t48SXsxcIy7r4huvPiJu7cxsxJgf3ffGrV/7O55ZrYG6Oju/0mYRz7hVufdo++XAw3d/X/NbBawkXD31UcTnqshknHqcYhkjlfwuaJxkvlPwuftlB2XHAncAgwCFpqZjldKtVFwiGTOaQnv/4o+v0S4syrAeOCf0ec5wAWw4+FKLSuaqZk1ADq5+/OEhwq1Anbp9Yhkiv6XIlI1TRLuxAswy91LT8ndy8z+TfgP2rio7WLgbjP7KbCGsruUXgLcbmY/IPQsLgAquu11DjDVzPYmPKDn99HzNESqhY5xiGRAdIyjwN1Lsl2LSLppV5WIiMSiHoeIiMSiHoeIiMSi4BARkVgUHCIiEouCQ0REYlFwiIhILP8fa5WTvzJCjHgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#위의 히스토리를 그래프로 그려보자\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accu = history_dict['accuracy']\n",
    "value_accu = history_dict['val_accuracy']\n",
    "\n",
    "loss = history_dict['loss']\n",
    "value_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(accu)+1)\n",
    "\n",
    "# bo와 b는 각각 파란 점선, 파란 선\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n",
    "plt.plot(epochs, value_loss, 'b', label = 'Validation Loss')\n",
    "\n",
    "#그래프 주석(타이틀, x축, y축)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그래프를 그려 확인할 때, validation loss와 training loss 의 차이가 커지면 트레이닝이 무의미해진다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, accu, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, value_accu, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 워드 임베딩 기법과 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 우리는 Enbedding 레이어를 통해 10,000개의 크기를 가진 파라미터를 학습시켰다.    \n",
    "   * gensim 패키지를 설치하여 몇 가지를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# 아까 옵션을 주어 10,000 길이를 다운로드 받았다.\n",
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장 \n",
    "word2vec_file_path = os.getenv('HOME')+'/SUBMIT_MISSION_GIT/ex4_Text/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "\n",
    "#몇 개, 얼마만큼의 사이즈를 적용할 것인지\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  \n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0216671 , -0.04035783, -0.02801956,  0.01957939, -0.02708866,\n",
       "        0.04285479, -0.04346569,  0.00972694,  0.00131848,  0.00091889,\n",
       "       -0.0238007 , -0.03882904, -0.02587099,  0.04241561,  0.02553775,\n",
       "       -0.0223966 ], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "# Computer라는 단어를 읽어 해당 단어의 워드 벡터를 보여준다.\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrific', 0.8653486967086792),\n",
       " ('subtle', 0.8570706844329834),\n",
       " ('shred', 0.8505549430847168),\n",
       " ('persons', 0.827022135257721),\n",
       " ('liberty', 0.8180781602859497),\n",
       " ('favourite', 0.8018106818199158),\n",
       " ('actor', 0.7980704307556152),\n",
       " ('goers', 0.7953050136566162),\n",
       " ('animated', 0.7910274863243103),\n",
       " ('reality', 0.7894230484962463)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#기학습된 데이터 중 'love'와 비슷한 게 있을까?\n",
    "word_vectors.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전이학습과 Word2Vec     \n",
    "\n",
    "* 왜 중요한가?    \n",
    "> Word2Vec과 같은 사전학습된 모델은 '이전에 학습된 데이터를 누적시켜 적용' 할 수 있기 때문에, 제로베이스에서 시작하는 다른 모델보다 정확도가 높게 나올 확률이 크다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec를 가져와서 동일한 'computer'의 워드벡터를 확인해보자.\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = os.getenv('HOME')+'/SUBMIT_MISSION_GIT/ex4_Text/W2V/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['computer']\n",
    "vector     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.6907792091369629),\n",
       " ('adore', 0.6816873550415039),\n",
       " ('loves', 0.661863386631012),\n",
       " ('passion', 0.6100709438323975),\n",
       " ('hate', 0.600395679473877),\n",
       " ('loving', 0.5886635780334473),\n",
       " ('affection', 0.5664337873458862),\n",
       " ('undying_love', 0.5547305345535278),\n",
       " ('absolutely_adore', 0.5536840558052063),\n",
       " ('adores', 0.5440906882286072)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec를 사용하니, 아까보다는 결과값이 좀 더 'love'와 가까워진 것을 확인할 수 있다.\n",
    "word2vec.similar_by_word(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,000개의 단어\n",
    "vocab_size = 10000 \n",
    "# 워드 벡터의 차원수(변경가능, Word2Vec 설명에 300개의 차원수라고 기재되어 있었음)\n",
    "word_vector_dim = 300  \n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 변경 및 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 580, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 574, 16)           33616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 114, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 108, 16)           1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 3,035,569\n",
      "Trainable params: 3,035,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    \n",
    "word_vector_dim = 300  \n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix), \n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))  \n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 21s 700ms/step - loss: 0.6817 - accuracy: 0.5570 - val_loss: 0.6646 - val_accuracy: 0.6033\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 9s 301ms/step - loss: 0.6189 - accuracy: 0.6801 - val_loss: 0.5681 - val_accuracy: 0.7381\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 9s 300ms/step - loss: 0.4627 - accuracy: 0.8108 - val_loss: 0.4019 - val_accuracy: 0.8263\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 9s 298ms/step - loss: 0.3213 - accuracy: 0.8674 - val_loss: 0.3245 - val_accuracy: 0.8635\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 9s 300ms/step - loss: 0.2315 - accuracy: 0.9137 - val_loss: 0.2966 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 10s - loss: 0.3088 - accuracy: 0.8692\n",
      "[0.30877816677093506, 0.8692399859428406]\n"
     ]
    }
   ],
   "source": [
    "#모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 아까 단순히 테스트했던 정확도(0.50대)에서, 많이 향상된 것을 확인할 수 있다. (정확도 +0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그럼 이제 '한국어 감성분석 실습'을 진행해보자."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
